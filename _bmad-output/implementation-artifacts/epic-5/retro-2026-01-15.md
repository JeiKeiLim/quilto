# Epic 5 Retrospective - Human-in-the-Loop

**Date:** 2026-01-15
**Facilitator:** Bob (Scrum Master)
**Epic Status:** In Progress (4/4 original stories done, 1 hotfix added)

---

## Team Participants

- Alice (Product Owner)
- Bob (Scrum Master) - Facilitator
- Charlie (Senior Dev)
- Dana (QA Engineer)
- Elena (Junior Dev)
- Jongkuk Lim (Project Lead)

---

## Epic Summary

**Epic 5: Human-in-the-Loop**

| Metric | Value |
|--------|-------|
| Original Stories Completed | 4/4 (100%) |
| Hotfix Stories Added | 1 (Story 5.5) |
| Tests Written | ~200+ new tests |
| New Modules | 2 (quilto.state, quilto.flow) |
| New Agent | 1 (ClarifierAgent) |
| Domain Enhancements | 4 (clarification_patterns for all fitness domains) |
| Code Review Cycles | 4 |
| Production Incidents | 0 |

### Stories Delivered

| Story | Deliverable | Tests | Review Fixes |
|-------|-------------|-------|--------------|
| 5-1 | ClarifierAgent (question generation) | 55 | 2 |
| 5-2 | WAIT_USER state + SessionState + routing | 33 | 3 |
| 5-3 | Correction flow (append + upsert) | 28 | 2 |
| 5-4 | Fitness clarification patterns | ~20 | 0 |
| 5-5 | Evaluator clarification context (HOTFIX) | TBD | TBD |

### FRs Covered

- FR-F13: Request missing info - human-in-the-loop (Clarifier)
- FR-F19: Handle corrections with append strategy

---

## Epic 4 Retrospective Action Item Follow-Through

| # | Action Item | Status | Evidence |
|---|-------------|--------|----------|
| 1 | Create and complete Story 3.5: Improve Retrieval Strategy Ordering | ✅ Completed | sprint-status shows done |
| 2 | Document dual LLM support in project-context.md | ✅ Completed | Section added to project-context.md |
| 3 | Add retrospective action item tracking to sprint-status.yaml | ⏳ Partial | Comments added, no formal section |

**Follow-Through Assessment:** ~83% (2.5/3) - Improved from Epic 4's 50%

---

## What Went Well

### Successes

1. **100% original story completion** with zero production incidents
2. **Strong test coverage** - 200+ new tests across 4 stories
3. **Clean agent pattern reuse** - ClarifierAgent followed established patterns
4. **New infrastructure modules** - quilto.state and quilto.flow created
5. **Domain-specific clarification patterns** - contextually appropriate questions
6. **Manual testing caught critical bug** - Evaluator missing clarification context

### Breakthrough Moments

- **Story 5-1:** Non-retrievable gap filtering (SUBJECTIVE, CLARIFICATION only)
- **Story 5-2:** LangGraph `interrupt()` for human-in-the-loop pause
- **Story 5-3:** Correction flow with append (raw) + upsert (parsed) semantics
- **Story 5-4:** `clarification_patterns` field extends DomainModule interface

### Patterns That Worked

| Pattern | Story | Impact |
|---------|-------|--------|
| Agent pattern reuse | 5-1 | Fast implementation following established patterns |
| TypedDict + Annotated reducers | 5-2 | LangGraph-ready state management |
| Append + upsert for corrections | 5-3 | Audit trail preserved, data updated |
| Domain-specific config injection | 5-4 | Framework + app separation maintained |
| Manual end-to-end testing | Retro | Caught Evaluator context bug |

---

## Significant Discovery

### Evaluator Missing Clarification Context (CRITICAL)

**Problem:** During manual testing, discovered that the Evaluator agent does not receive user clarification responses. This causes valid responses to be incorrectly flagged as "insufficient."

**Example from manual_test.log:**
1. Clarifier asked: "What is your current flat-bench press 1RM?"
2. User answered: "Haven't measured yet. 60kg 10 reps so far."
3. Synthesizer correctly used this to estimate ~80kg 1RM
4. Evaluator flagged: "Speculative estimate of 1RM not grounded in user's recorded lifts"
5. Evaluator suggested: "Ask the user for their current flat-bench max" - **but we already asked!**

**Root Cause:**
- `EvaluatorInput` model missing `user_responses` field
- `Evaluator.build_prompt()` has no awareness of clarification context
- Evaluator treats user-provided data as speculation

**Impact:**
- Valid responses incorrectly fail evaluation
- Retry loop wastes LLM calls on correct responses
- User experience degraded

**Resolution:** Story 5.5 created as hotfix (similar to Story 3.5 from Epic 4)

---

## Key Learnings

1. **Manual testing catches integration bugs** - Unit tests passed, but end-to-end flow revealed Evaluator context gap
2. **Agent pattern is mature** - ClarifierAgent implemented quickly by following established patterns
3. **State infrastructure pays off** - SessionState + quilto.state module ready for full LangGraph integration
4. **Domain-specific patterns enhance quality** - clarification_patterns makes questions contextually appropriate
5. **Hotfix stories are healthy agile practice** - Story 3.5 (Epic 4) and Story 5.5 (Epic 5) show the pattern works
6. **Context must flow through entire pipeline** - Clarification context needed by Synthesizer AND Evaluator

---

## Epic 6 Preview

### Epic 6: Domain Intelligence

| Story | Description | Key Pattern |
|-------|-------------|-------------|
| 6-1 | Implement Domain Auto-Selection | Router selects relevant domains |
| 6-2 | Implement Multi-Domain Combination | Merge vocabularies, expertise |
| 6-3 | Implement Mid-Flow Domain Expansion | Planner/Analyzer request domains |
| 6-4 | Create Swimming Domain Module | New fitness subdomain |

### Dependencies on Epic 5

- SessionState for tracking active domains
- Clarifier patterns for domain-specific questions
- WAIT_USER state if domain expansion needs user input

---

## Action Items

| # | Action Item | Owner | Priority | Deadline |
|---|-------------|-------|----------|----------|
| 1 | Complete Story 5.5: Pass clarification context to Evaluator | Dev | CRITICAL | Before Epic 6 |
| 2 | Run manual_test.py after Story 5.5 to verify fix | QA | HIGH | After 5.5 |
| 3 | Continue running manual_test.py after each epic | Team | MEDIUM | Ongoing |

---

## Technical Debt

| Item | Severity | Notes |
|------|----------|-------|
| Evaluator missing clarification context | HIGH | Addressed by Story 5.5 |
| Pyright/Pydantic `default_factory` workaround | LOW | Known issue, documented |

---

## Team Agreements

1. **Run manual_test.py after each epic** - Catches integration bugs that unit tests miss
2. **Hotfix stories for critical bugs** - Better than carrying bugs into next epic
3. **Context must flow through entire pipeline** - All agents that evaluate need full context
4. **Continue code review practice** - 4/4 stories had review cycles with fixes

---

## Retrospective Outcome

| Item | Status |
|------|--------|
| Epic 5 Review | Complete (original 4 stories) |
| Previous Retro Follow-Through | ~83% (improved from 50%) |
| Significant Discovery | Evaluator missing clarification context |
| Hotfix Story Created | Story 5.5 (ready-for-dev) |
| Action Items | 3 captured |
| Technical Debt | 2 items (1 HIGH addressed by 5.5) |
| Team Agreements | 4 captured |
| Next Epic Ready | After Story 5.5 completion |

---

## Critical Path Before Epic 6

| Task | Owner | Status |
|------|-------|--------|
| Story 5.5: Pass clarification context to Evaluator | Dev | ready-for-dev |
| Verify fix via manual_test.py | QA | Pending |
| Mark Epic 5 as done | SM | After 5.5 completion |

---

## Closing Notes

**Jongkuk Lim (Project Lead):** "The manual testing approach is proving its value. Story 5.5 is exactly the kind of bug that would have caused problems in production."

**Team Consensus:** Epic 5 delivered solid human-in-the-loop infrastructure with the Clarifier agent, WAIT_USER state, and correction flow. The bug discovered during manual testing validates the importance of end-to-end validation. Story 5.5 will complete the clarification flow by ensuring the Evaluator has proper context.

---

*Retrospective facilitated by Bob (Scrum Master)*
*Document generated: 2026-01-15*
