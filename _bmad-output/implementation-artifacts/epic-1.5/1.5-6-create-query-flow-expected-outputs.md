# Story 1.5-6: Create Query Flow Expected Outputs

Status: done

## Story

As a **Quilto developer**,
I want **expected outputs for end-to-end query testing**,
So that **full query flow accuracy can be validated (Epic 3-4 preparation)**.

## Acceptance Criteria

1. **Test cases stored in correct location** - All query test cases are created in `tests/corpus/fitness/expected/query/` directory

2. **Test case structure defined** - Each test case is a JSON file with `query`, `context_entries`, `expected_analysis_points`, `expected_response_elements` fields as specified in epics.md

3. **Simple queries covered** - Test cases include straightforward single-question queries (e.g., "How has my bench press progressed?")

4. **Complex multi-part queries covered** - Test cases include queries requiring multiple sub-analyses (e.g., "Compare my deadlift to my squat progression over the past year")

5. **Insufficient data queries covered** - Test cases include queries where the expected response should acknowledge data gaps

6. **Test cases use existing entries** - All `context_entries` reference actual dates from `from_csv/` and `synthetic/` entries

7. **At least 10 query test cases created** - Minimum count per epics.md requirements

8. **Format allows fuzzy matching** - Expected outputs use key points (analysis_points, response_elements) rather than exact text

9. **Validation test created** - pytest test validates JSON structure and that referenced entry IDs exist

## Tasks / Subtasks

- [x] Task 1: Create directory structure and schema (AC: #1, #2, #8)
  - [x] 1.1 Create `tests/corpus/fitness/expected/query/` directory
  - [x] 1.2 Create Pydantic schema `query_test_case.py` in `tests/corpus/schemas/`
  - [x] 1.3 Export schema from `__init__.py`

- [x] Task 2: Create simple query test cases (AC: #3, #6)
  - [x] 2.1 Create test case: "How has my bench press progressed?" (context: 2019-01-29, 2019-02-01, 2019-03-12)
  - [x] 2.2 Create test case: "What was my heaviest deadlift?" (context: 2019-01-28 has 190kg trap bar)
  - [x] 2.3 Create test case: "Show my workout frequency in September 2019" (context: all 16 Sep 2019 entries)

- [x] Task 3: Create complex multi-part query test cases (AC: #4, #6)
  - [x] 3.1 Create test case: "Compare my deadlift to squat progression" (context: deadlift+squat overlap dates like 2019-04-09, 2019-04-17)
  - [x] 3.2 Create test case: "How do my push exercises compare to pull exercises?" (context: bench+press vs row entries)
  - [x] 3.3 Create test case: "What's my strongest lift and how has it improved?" (context: 2019-01-28 deadlift 190kg, bench/squat dates)
  - [x] 3.4 Create test case: "Analyze my training volume trends over the first quarter of 2019" (context: Jan-Mar 2019, 27 entries)

- [x] Task 4: Create insufficient data query test cases (AC: #5, #6)
  - [x] 4.1 Create test case: "How is my cardio improving?" (context: strength-only entries, expect gap acknowledgment)
  - [x] 4.2 Create test case: "Compare my bench press to last year" (context: entries from single month only)
  - [x] 4.3 Create test case: "What's my recovery pattern?" (context: entries without recovery/rest data)

- [x] Task 5: Validate test cases (AC: #7, #9)
  - [x] 5.1 Create pytest test `tests/corpus/test_query_expected.py`
  - [x] 5.2 Validate all JSON files parse correctly against schema
  - [x] 5.3 Validate all `context_entries` reference existing entries
  - [x] 5.4 Verify minimum 10 test cases exist
  - [x] 5.5 Run `uv run pyright` and `uv run ruff check .`

## Dev Notes

### Schema Design

Based on the epics.md specification, create this schema with Literal type constraints for vocabulary enforcement:

```python
# tests/corpus/schemas/query_test_case.py
from typing import Literal

from pydantic import BaseModel, ConfigDict

AnalysisPointType = Literal[
    "weight_progression_identified",
    "rep_range_noted",
    "time_span_mentioned",
    "volume_analysis",
    "frequency_pattern",
    "strength_comparison",
    "data_gap_identified",
    "peak_performance_found",
    "trend_direction",
    "exercise_variation_noted",
]

ResponseElementType = Literal[
    "mentions_starting_weight",
    "mentions_current_weight",
    "provides_trend_assessment",
    "includes_specific_dates",
    "acknowledges_data_limitation",
    "suggests_next_steps",
    "compares_exercises",
    "quantifies_improvement",
    "mentions_consistency",
]


class QueryTestCase(BaseModel):
    """Expected output for end-to-end query flow testing.

    Used to validate query accuracy in Epic 3 (Story 3-2: Implement Planner Agent)
    and Epic 4 (Stories 4-1, 4-2: Analysis & Response).

    Attributes:
        query: Natural language query from user.
        context_entries: List of entry IDs (dates) that provide context for the query.
        expected_analysis_points: List of key analysis points that should be identified.
        expected_response_elements: List of elements that should appear in the response.
    """

    model_config = ConfigDict(strict=True)

    query: str
    context_entries: list[str]
    expected_analysis_points: list[AnalysisPointType]
    expected_response_elements: list[ResponseElementType]
```

### Key Analysis Points Vocabulary

Use these standardized analysis point identifiers (following epics.md example):
- `weight_progression_identified` - Weight increase/decrease over time noted
- `rep_range_noted` - Rep ranges analyzed
- `time_span_mentioned` - Time period of data covered
- `volume_analysis` - Total sets/reps analyzed
- `frequency_pattern` - Workout frequency identified
- `strength_comparison` - Relative strength between exercises
- `data_gap_identified` - Missing data acknowledged
- `peak_performance_found` - Maximum/best performance identified
- `trend_direction` - Upward/downward/stable trend noted
- `exercise_variation_noted` - Different exercise variants compared

### Key Response Elements Vocabulary

Use these standardized response element identifiers:
- `mentions_starting_weight` - Starting weight referenced
- `mentions_current_weight` - Current/ending weight referenced
- `provides_trend_assessment` - Overall trend description
- `includes_specific_dates` - Specific dates mentioned
- `acknowledges_data_limitation` - Explicitly notes insufficient data
- `suggests_next_steps` - Provides recommendations (if applicable)
- `compares_exercises` - Direct exercise-to-exercise comparison
- `quantifies_improvement` - Numeric improvement percentage/amount
- `mentions_consistency` - Notes workout regularity

### Available Fitness Entries for Context

**Verified entries from corpus analysis (cross-referenced with parsed JSON):**

**Bench Press entries (barbell/dumbbell/incline variants):**
- 2019-01-29, 2019-02-01, 2019-02-27, 2019-03-12, 2019-03-15, 2019-03-19, 2019-03-25
- 2019-04-01, 2019-04-10, 2019-04-22, 2019-05-05, 2019-05-16, 2019-05-27, 2019-05-29
- 2019-07-29, 2019-08-05, 2019-08-30, 2019-09-03, 2019-09-06, 2019-09-10, 2019-09-17
- 2019-09-21, 2019-09-24, 2019-09-27, 2019-10-15, 2019-10-22, 2019-10-30, 2019-11-07

**Deadlift entries (barbell/sumo/trap bar variants):**
- 2019-01-28 (Trap Bar, up to 190kg), 2019-02-04 (Sumo), 2019-02-25, 2019-03-11 (Sumo)
- 2019-03-15, 2019-03-18 (Sumo), 2019-04-09, 2019-04-17 (Sumo), 2019-05-27
- 2019-07-31, 2019-08-12 (Sumo), 2019-09-02 (Sumo, up to 140kg), 2019-09-09, 2019-09-26
- 2019-10-17, 2019-11-12, 2022-02-08, 2022-03-27, 2022-04-15, 2022-05-24, 2022-07-02, 2023-03-20, 2023-08-31

**Squat entries (barbell/front/overhead variants):**
- 2019-01-31 (Front), 2019-02-11 (Front), 2019-02-18 (Front), 2019-02-28 (Front)
- 2019-03-08, 2019-03-29 (Front), 2019-04-09 (Front), 2019-04-17, 2019-04-23
- 2019-05-15, 2019-05-20 (Front), 2019-05-29 (Front), 2019-07-26 (Front), 2019-08-07
- 2019-09-05 (Front), 2019-09-19, 2019-09-23, 2019-09-30, 2019-10-14, 2019-10-21 (Front)

**September 2019 (high density - 16 entries):** 2019-09-02, 2019-09-03, 2019-09-05, 2019-09-06, 2019-09-09, 2019-09-10, 2019-09-16, 2019-09-17, 2019-09-18, 2019-09-19, 2019-09-21, 2019-09-23, 2019-09-24, 2019-09-26, 2019-09-27, 2019-09-30

**Q1 2019 (good time span):** Jan (3), Feb (12), Mar (12) = 27 entries

### Test Case JSON Format

Follow this format for each test case (per epics.md):

```json
{
  "query": "How has my bench press progressed?",
  "context_entries": ["2019-01-29", "2019-02-01", "2019-03-12"],
  "expected_analysis_points": [
    "weight_progression_identified",
    "rep_range_noted",
    "time_span_mentioned"
  ],
  "expected_response_elements": [
    "mentions_starting_weight",
    "mentions_current_weight",
    "provides_trend_assessment"
  ]
}
```

**Note:** All `context_entries` dates have been verified against the corpus. Only use dates from the "Available Fitness Entries" section above.

### File Naming Convention

Name files descriptively:
- `simple-bench-progression.json`
- `complex-deadlift-squat-compare.json`
- `insufficient-cardio-query.json`

### Project Structure Notes

- This is **Quilto framework** test infrastructure (domain-agnostic query flow testing)
- These test cases prepare for **Epic 3: Query & Retrieval** (Story 3.2: Implement Planner Agent) and **Epic 4: Analysis & Response** (Stories 4.1-4.2)
- Query expected outputs are for **fitness domain only** (domain-specific example data)
- Location: `tests/corpus/fitness/expected/query/` (per directory structure in epics.md)
- Fuzzy matching approach allows validation without exact text matching

### Previous Story Patterns (1.5-5 Learnings)

From Story 1.5-5 completion:
- Use `ConfigDict(strict=True)` for all Pydantic models
- Use Google-style docstrings with Attributes section
- Consider adding `Literal` type constraints for standardized vocabularies
- Export new schemas from `tests/corpus/schemas/__init__.py`
- Validation tests should check: schema parsing, entry existence, minimum count
- Update `__init__.py` docstring to reflect broader scope

### Validation Test Requirements

The pytest validation test (`tests/corpus/test_query_expected.py`) must:
1. Load all JSON files from `tests/corpus/fitness/expected/query/`
2. Validate each JSON parses against `QueryTestCase` schema
3. Check that every `context_entry` references an existing entry in:
   - `tests/corpus/fitness/entries/from_csv/{date}.md` OR
   - `tests/corpus/fitness/entries/synthetic/{id}.md`
4. Validate `expected_analysis_points` and `expected_response_elements` are non-empty lists
5. Count total test cases and assert >= 10
6. Report clear error messages for missing entries or schema violations
7. Include robustness tests (similar to `TestRetrievalTestCaseRobustness`):
   - Test that invalid analysis point types are rejected by Literal constraint
   - Test that invalid response element types are rejected by Literal constraint
   - Test that empty JSON object fails validation

### Validation Command

After creating outputs:
```bash
uv run ruff check . && uv run pyright && uv run pytest tests/corpus/test_query_expected.py -v
```

### Schema Integration

After creating `query_test_case.py`, add to `tests/corpus/schemas/__init__.py`:
```python
from tests.corpus.schemas.query_test_case import (
    AnalysisPointType,
    QueryTestCase,
    ResponseElementType,
)

__all__ = [
    # ... existing exports ...
    "AnalysisPointType",
    "QueryTestCase",
    "ResponseElementType",
]
```

**Also update the `__init__.py` docstring** to reflect broader scope (parser, retrieval, and query schemas):
```python
"""Test schemas for expected outputs.

This module exports Pydantic models for validating expected output
JSON files in the test corpus, including parser outputs, retrieval test cases,
and query flow test cases.
"""
```

### Query Flow Coverage Matrix

| Query Type | Count | File Prefix | Examples |
|------------|-------|-------------|----------|
| Simple single-question | 3 | `simple-*` | bench progression, heaviest deadlift, workout frequency |
| Complex multi-part | 4 | `complex-*` | deadlift vs squat, push vs pull, strongest lift, volume trends |
| Insufficient data | 3 | `insufficient-*` | cardio progress, year comparison, recovery pattern |
| **Total** | **10** | | Per AC #7 minimum |

### References

- [Source: _bmad-output/planning-artifacts/epics.md#Story 1.5-6] - Full acceptance criteria and JSON format example
- [Source: tests/corpus/schemas/retrieval_test_case.py] - Schema patterns (ConfigDict, Literal types, Google docstrings)
- [Source: tests/corpus/schemas/__init__.py] - Export pattern for new schemas
- [Source: tests/corpus/fitness/entries/from_csv/] - Available entry dates for context_entries (93 entries)
- [Source: tests/corpus/fitness/expected/retrieval/*.json] - Exercise data to identify which entries have which exercises
- [Source: _bmad-output/implementation-artifacts/epic-1.5/1.5-5-create-retrieval-expected-outputs.md] - Previous story patterns and learnings

## Dev Agent Record

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References

N/A

### Completion Notes List

- Created QueryTestCase schema with Literal type constraints for AnalysisPointType and ResponseElementType
- Updated tests/corpus/schemas/__init__.py with new exports and broader docstring
- Created 10 query test cases: 3 simple, 4 complex, 3 insufficient data
- Fixed Q1 volume trends context_entries to use only verified existing dates
- All 258 tests pass, pyright 0 errors, ruff clean

### File List

- tests/corpus/schemas/query_test_case.py (new)
- tests/corpus/schemas/__init__.py (modified)
- tests/corpus/fitness/expected/query/simple-bench-progression.json (new)
- tests/corpus/fitness/expected/query/simple-heaviest-deadlift.json (new)
- tests/corpus/fitness/expected/query/simple-september-frequency.json (new)
- tests/corpus/fitness/expected/query/complex-deadlift-squat-compare.json (new)
- tests/corpus/fitness/expected/query/complex-push-vs-pull.json (new)
- tests/corpus/fitness/expected/query/complex-strongest-lift.json (new)
- tests/corpus/fitness/expected/query/complex-q1-volume-trends.json (new)
- tests/corpus/fitness/expected/query/insufficient-cardio-progress.json (new)
- tests/corpus/fitness/expected/query/insufficient-year-comparison.json (new)
- tests/corpus/fitness/expected/query/insufficient-recovery-pattern.json (new)
- tests/corpus/test_query_expected.py (new)

## Senior Developer Review (AI)

### Review Summary

**Reviewer:** Amelia (Dev Agent)
**Date:** 2026-01-09
**Outcome:** APPROVED

### Findings

**0 High, 2 Medium (fixed), 0 Low**

All critical acceptance criteria validated:
- [x] AC #1: Test cases stored in `tests/corpus/fitness/expected/query/` ✓
- [x] AC #2: Test case structure follows schema (query, context_entries, expected_analysis_points, expected_response_elements) ✓
- [x] AC #3: Simple queries covered (3 test cases) ✓
- [x] AC #4: Complex multi-part queries covered (4 test cases) ✓
- [x] AC #5: Insufficient data queries covered (3 test cases) ✓
- [x] AC #6: All context_entries reference existing entries from from_csv/synthetic ✓
- [x] AC #7: At least 10 query test cases exist (10 found) ✓
- [x] AC #8: Format allows fuzzy matching via analysis_points/response_elements ✓
- [x] AC #9: Validation test created (`tests/corpus/test_query_expected.py`) ✓

### Fixes Applied

1. **M1 (Fixed):** Updated module docstring to document 5 verification areas instead of 4
2. **M2 (Fixed):** Added file names to error messages in `TestQueryTestCaseCoverage` tests for better debugging

### Validation Results

```
uv run ruff check . - All checks passed!
uv run pyright - 0 errors, 0 warnings
uv run pytest tests/corpus/test_query_expected.py -v - 12 passed
```

### Change Log Entry

- 2026-01-09: Code review completed, 2 medium issues fixed, status → done

